{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrB-hQaiVUP2"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qF3RTMEVVwH",
        "outputId": "8ba7edf2-a83b-46a8-e9ab-cea11b44b63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFhMTxV2VXIa"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle/ \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xgzzz4uVY9B"
      },
      "outputs": [],
      "source": [
        "! cp '/content/drive/MyDrive/kaggle.json' ~/.kaggle/ \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCpYstNGVcZu"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmmZjcncVedm",
        "outputId": "f52c9d84-74d6-4c36-961c-e40ae503f3c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                           title                                               size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "meirnizri/covid19-dataset                                     COVID-19 Dataset                                     5MB  2022-11-13 15:47:17          15715        444  1.0              \n",
            "devrimtuner/list-of-moststreamed-songs-on-spotify             Top 100 Spotify Songs👑🎤🎧🎼                            3KB  2022-12-30 05:42:54            609         39  1.0              \n",
            "thedevastator/analyzing-credit-card-spending-habits-in-india  Credit Card Spending Habits in India               319KB  2022-12-14 07:30:37           1884         63  1.0              \n",
            "die9origephit/fifa-world-cup-2022-complete-dataset            Fifa World Cup 2022: Complete Dataset                7KB  2022-12-18 22:51:11           3656        127  1.0              \n",
            "michals22/coffee-dataset                                      Coffee dataset                                      24KB  2022-12-15 20:02:12           3997         90  1.0              \n",
            "heemalichaudhari/netflix-movies-and-series                    Netflix Movies and Series                            2MB  2022-12-22 13:34:22           1227         32  0.8235294        \n",
            "sejungjenn/spotify-best-songs-of-2022                         Spotify: Winner Tracks Audio Features🎹              38KB  2022-12-28 08:06:49            316         22  1.0              \n",
            "thedevastator/unlock-profits-with-e-commerce-sales-data       E-Commerce Sales Dataset                             6MB  2022-12-03 09:27:17           3272         79  1.0              \n",
            "aklimarimi/qs-world-ranked-universities-20182022              QS World ranked Universities (2018-2022)            51KB  2022-12-28 03:53:39            651         32  1.0              \n",
            "rajeshrampure/black-friday-sale                               Black Friday Sale                                    5MB  2022-12-24 09:37:49           1030         32  1.0              \n",
            "devrimtuner/highestpaid-athletes                              HIGHEST-PAID ATHLETES⚽️🏀🏈⚾️🥎🎾                        1KB  2022-12-29 01:29:51            312         27  1.0              \n",
            "heemalichaudhari/shopping                                     Shopping                                            12KB  2022-12-26 14:25:07            519         28  0.9411765        \n",
            "milanvaddoriya/old-car-price-prediction                       Old car price prediction                           105KB  2022-12-24 15:38:56            491         30  1.0              \n",
            "thedevastator/how-does-daily-yoga-impact-screen-time-habits   How Does Daily Yoga Impact Screen Time Habits       742B  2022-12-14 04:10:56            739         24  1.0              \n",
            "thedevastator/uncovering-factors-that-affect-used-car-prices  Used Cars                                           18MB  2022-12-06 13:36:08           1197         37  1.0              \n",
            "devrimtuner/list-of-mostfollowed-instagram-accounts           (TOP 50)List of most-followed Instagram accounts👑    2KB  2022-12-30 07:52:00            366         27  1.0              \n",
            "thedevastator/jobs-dataset-from-glassdoor                     Salary Prediction                                    3MB  2022-11-16 13:52:31           8569        182  1.0              \n",
            "dansbecker/melbourne-housing-snapshot                         Melbourne Housing Snapshot                         451KB  2018-06-05 12:52:24         103466       1191  0.7058824        \n",
            "mattop/best-selling-game-boy-video-games                      Best Selling Game Boy Video Games                    2KB  2022-12-17 18:41:38            395         27  0.9705882        \n",
            "rajeshrampure/zomato-dataset                                  Zomato Dataset                                      89MB  2022-12-23 07:38:07            631         28  1.0              \n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky18Ur8_WXVW",
        "outputId": "845868d4-97fa-4115-86b4-9167269edd9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.9.2\n",
            "Uninstalling tensorflow-2.9.2:\n",
            "  Successfully uninstalled tensorflow-2.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow --yes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duheTztBWXy2",
        "outputId": "52fc8972-d7b7-46a1-e23c-19024b212cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp38-cp38-manylinux2010_x86_64.whl (516.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.6/454.6 KB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (2.1.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.51.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (0.38.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Collecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.2.0) (3.19.6)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.25.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.9)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (5.2.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, scipy, h5py, gast, cachetools, google-auth, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.2.0\n",
            "    Uninstalling cachetools-5.2.0:\n",
            "      Successfully uninstalled cachetools-5.2.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.15.0\n",
            "    Uninstalling google-auth-2.15.0:\n",
            "      Successfully uninstalled google-auth-2.15.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.4.0 requires scipy>=1.6, but you have scipy 1.4.1 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.25 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "google-api-core 2.11.0 requires google-auth<3.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cachetools-4.2.4 gast-0.3.3 google-auth-1.35.0 h5py-2.10.0 scipy-1.4.1 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OfYNI0rWZfz",
        "outputId": "9e5a8c02-35f5-436b-b3bb-5dfd50edd873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version:  2.2.0\n",
            "Eager mode:  True\n",
            "GPU is NOT AVAILABLE\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import pandas  as pd\n",
        "import numpy   as np\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from zipfile import ZipFile\n",
        "from shutil import copyfile, copyfileobj\n",
        "import gzip\n",
        "from IPython.display import clear_output\n",
        "import cv2\n",
        "import os\n",
        "from pylab import rcParams\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import scipy\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, MeanShift\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNXqMiTZWcrP"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import math\n",
        "import seaborn as sns; sns.set()\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "from itertools import chain\n",
        "from skimage.io import imread, imshow, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
        "from tensorflow.keras.layers import Lambda, RepeatVector, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import MaxPooling2D, GlobalMaxPool2D\n",
        "from tensorflow.keras.layers import concatenate, add\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from tensorflow.keras import backend as K\n",
        "import joblib\n",
        "import gc\n",
        "#import segmentation_models as sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV6WCcXQWe44",
        "outputId": "d844c2df-8db0-4fa2-943f-250654464069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading covid19-ct-scans.zip to /content\n",
            "100% 1.03G/1.03G [00:52<00:00, 19.1MB/s]\n",
            "100% 1.03G/1.03G [00:52<00:00, 21.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d andrewmvd/covid19-ct-scans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7X0tXVwWkmc",
        "outputId": "7db2f804-c2c5-4a4a-dc73-567e92aeb028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/covid19-ct-scans.zip\n",
            "  inflating: database-data/ct_scans/coronacases_org_001.nii  \n",
            "  inflating: database-data/ct_scans/coronacases_org_002.nii  \n",
            "  inflating: database-data/ct_scans/coronacases_org_003.nii  \n",
            "  inflating: database-data/ct_scans/coronacases_org_004.nii  \n",
            "  inflating: database-data/ct_scans/coronacases_org_005.nii  \n",
            "  inflating: database-data/ct_scans/coronacases_org_006.nii  \n",
            "  inflating: database-data/ct_scans/coronacases_org_007.nii  \n",
            "  inflating: database-data/ct_scans/coronacases_org_008.nii  \n",
            "  inflating: database-data/ct_scans/coronacases_org_009.nii  \n",
            "  inflating: database-data/ct_scans/coronacases_org_010.nii  \n",
            "  inflating: database-data/ct_scans/radiopaedia_org_covid-19-pneumonia-10_85902_1-dcm.nii  \n",
            "  inflating: database-data/ct_scans/radiopaedia_org_covid-19-pneumonia-10_85902_3-dcm.nii  \n",
            "  inflating: database-data/ct_scans/radiopaedia_org_covid-19-pneumonia-14_85914_0-dcm.nii  \n",
            "  inflating: database-data/ct_scans/radiopaedia_org_covid-19-pneumonia-27_86410_0-dcm.nii  \n",
            "  inflating: database-data/ct_scans/radiopaedia_org_covid-19-pneumonia-29_86490_1-dcm.nii  \n",
            "  inflating: database-data/ct_scans/radiopaedia_org_covid-19-pneumonia-29_86491_1-dcm.nii  \n",
            "  inflating: database-data/ct_scans/radiopaedia_org_covid-19-pneumonia-36_86526_0-dcm.nii  \n",
            "  inflating: database-data/ct_scans/radiopaedia_org_covid-19-pneumonia-40_86625_0-dcm.nii  \n",
            "  inflating: database-data/ct_scans/radiopaedia_org_covid-19-pneumonia-4_85506_1-dcm.nii  \n",
            "  inflating: database-data/ct_scans/radiopaedia_org_covid-19-pneumonia-7_85703_0-dcm.nii  \n",
            "  inflating: database-data/infection_mask/coronacases_001.nii  \n",
            "  inflating: database-data/infection_mask/coronacases_002.nii  \n",
            "  inflating: database-data/infection_mask/coronacases_003.nii  \n",
            "  inflating: database-data/infection_mask/coronacases_004.nii  \n",
            "  inflating: database-data/infection_mask/coronacases_005.nii  \n",
            "  inflating: database-data/infection_mask/coronacases_006.nii  \n",
            "  inflating: database-data/infection_mask/coronacases_007.nii  \n",
            "  inflating: database-data/infection_mask/coronacases_008.nii  \n",
            "  inflating: database-data/infection_mask/coronacases_009.nii  \n",
            "  inflating: database-data/infection_mask/coronacases_010.nii  \n",
            "  inflating: database-data/infection_mask/radiopaedia_10_85902_1.nii  \n",
            "  inflating: database-data/infection_mask/radiopaedia_10_85902_3.nii  \n",
            "  inflating: database-data/infection_mask/radiopaedia_14_85914_0.nii  \n",
            "  inflating: database-data/infection_mask/radiopaedia_27_86410_0.nii  \n",
            "  inflating: database-data/infection_mask/radiopaedia_29_86490_1.nii  \n",
            "  inflating: database-data/infection_mask/radiopaedia_29_86491_1.nii  \n",
            "  inflating: database-data/infection_mask/radiopaedia_36_86526_0.nii  \n",
            "  inflating: database-data/infection_mask/radiopaedia_40_86625_0.nii  \n",
            "  inflating: database-data/infection_mask/radiopaedia_4_85506_1.nii  \n",
            "  inflating: database-data/infection_mask/radiopaedia_7_85703_0.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/coronacases_001.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/coronacases_002.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/coronacases_003.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/coronacases_004.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/coronacases_005.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/coronacases_006.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/coronacases_007.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/coronacases_008.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/coronacases_009.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/coronacases_010.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/radiopaedia_10_85902_1.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/radiopaedia_10_85902_3.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/radiopaedia_14_85914_0.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/radiopaedia_27_86410_0.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/radiopaedia_29_86490_1.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/radiopaedia_29_86491_1.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/radiopaedia_36_86526_0.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/radiopaedia_40_86625_0.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/radiopaedia_4_85506_1.nii  \n",
            "  inflating: database-data/lung_and_infection_mask/radiopaedia_7_85703_0.nii  \n",
            "  inflating: database-data/lung_mask/coronacases_001.nii  \n",
            "  inflating: database-data/lung_mask/coronacases_002.nii  \n",
            "  inflating: database-data/lung_mask/coronacases_003.nii  \n",
            "  inflating: database-data/lung_mask/coronacases_004.nii  \n",
            "  inflating: database-data/lung_mask/coronacases_005.nii  \n",
            "  inflating: database-data/lung_mask/coronacases_006.nii  \n",
            "  inflating: database-data/lung_mask/coronacases_007.nii  \n",
            "  inflating: database-data/lung_mask/coronacases_008.nii  \n",
            "  inflating: database-data/lung_mask/coronacases_009.nii  \n",
            "  inflating: database-data/lung_mask/coronacases_010.nii  \n",
            "  inflating: database-data/lung_mask/radiopaedia_10_85902_1.nii  \n",
            "  inflating: database-data/lung_mask/radiopaedia_10_85902_3.nii  \n",
            "  inflating: database-data/lung_mask/radiopaedia_14_85914_0.nii  \n",
            "  inflating: database-data/lung_mask/radiopaedia_27_86410_0.nii  \n",
            "  inflating: database-data/lung_mask/radiopaedia_29_86490_1.nii  \n",
            "  inflating: database-data/lung_mask/radiopaedia_29_86491_1.nii  \n",
            "  inflating: database-data/lung_mask/radiopaedia_36_86526_0.nii  \n",
            "  inflating: database-data/lung_mask/radiopaedia_40_86625_0.nii  \n",
            "  inflating: database-data/lung_mask/radiopaedia_4_85506_1.nii  \n",
            "  inflating: database-data/lung_mask/radiopaedia_7_85703_0.nii  \n",
            "  inflating: database-data/metadata.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/covid19-ct-scans.zip -d database-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrdMAtl8WsWD"
      },
      "outputs": [],
      "source": [
        "with ZipFile('covid19-ct-scans.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall('covid19-ct-scans')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "iVIAewgkWsuP",
        "outputId": "cd3d8159-5a49-4f71-9837-19a7c231821a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             ct_scan  \\\n",
              "0  /content/covid19-ct-scans/ct_scans/coronacases...   \n",
              "1  /content/covid19-ct-scans/ct_scans/coronacases...   \n",
              "2  /content/covid19-ct-scans/ct_scans/coronacases...   \n",
              "3  /content/covid19-ct-scans/ct_scans/coronacases...   \n",
              "4  /content/covid19-ct-scans/ct_scans/coronacases...   \n",
              "\n",
              "                                           lung_mask  \\\n",
              "0  /content/covid19-ct-scans/lung_mask/coronacase...   \n",
              "1  /content/covid19-ct-scans/lung_mask/coronacase...   \n",
              "2  /content/covid19-ct-scans/lung_mask/coronacase...   \n",
              "3  /content/covid19-ct-scans/lung_mask/coronacase...   \n",
              "4  /content/covid19-ct-scans/lung_mask/coronacase...   \n",
              "\n",
              "                                      infection_mask  \\\n",
              "0  /content/covid19-ct-scans/infection_mask/coron...   \n",
              "1  /content/covid19-ct-scans/infection_mask/coron...   \n",
              "2  /content/covid19-ct-scans/infection_mask/coron...   \n",
              "3  /content/covid19-ct-scans/infection_mask/coron...   \n",
              "4  /content/covid19-ct-scans/infection_mask/coron...   \n",
              "\n",
              "                             lung_and_infection_mask  \n",
              "0  /content/covid19-ct-scans/lung_and_infection_m...  \n",
              "1  /content/covid19-ct-scans/lung_and_infection_m...  \n",
              "2  /content/covid19-ct-scans/lung_and_infection_m...  \n",
              "3  /content/covid19-ct-scans/lung_and_infection_m...  \n",
              "4  /content/covid19-ct-scans/lung_and_infection_m...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2903c6fc-e563-443a-b4ee-001da66c14f3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ct_scan</th>\n",
              "      <th>lung_mask</th>\n",
              "      <th>infection_mask</th>\n",
              "      <th>lung_and_infection_mask</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/covid19-ct-scans/ct_scans/coronacases...</td>\n",
              "      <td>/content/covid19-ct-scans/lung_mask/coronacase...</td>\n",
              "      <td>/content/covid19-ct-scans/infection_mask/coron...</td>\n",
              "      <td>/content/covid19-ct-scans/lung_and_infection_m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/covid19-ct-scans/ct_scans/coronacases...</td>\n",
              "      <td>/content/covid19-ct-scans/lung_mask/coronacase...</td>\n",
              "      <td>/content/covid19-ct-scans/infection_mask/coron...</td>\n",
              "      <td>/content/covid19-ct-scans/lung_and_infection_m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/covid19-ct-scans/ct_scans/coronacases...</td>\n",
              "      <td>/content/covid19-ct-scans/lung_mask/coronacase...</td>\n",
              "      <td>/content/covid19-ct-scans/infection_mask/coron...</td>\n",
              "      <td>/content/covid19-ct-scans/lung_and_infection_m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/covid19-ct-scans/ct_scans/coronacases...</td>\n",
              "      <td>/content/covid19-ct-scans/lung_mask/coronacase...</td>\n",
              "      <td>/content/covid19-ct-scans/infection_mask/coron...</td>\n",
              "      <td>/content/covid19-ct-scans/lung_and_infection_m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/covid19-ct-scans/ct_scans/coronacases...</td>\n",
              "      <td>/content/covid19-ct-scans/lung_mask/coronacase...</td>\n",
              "      <td>/content/covid19-ct-scans/infection_mask/coron...</td>\n",
              "      <td>/content/covid19-ct-scans/lung_and_infection_m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2903c6fc-e563-443a-b4ee-001da66c14f3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2903c6fc-e563-443a-b4ee-001da66c14f3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2903c6fc-e563-443a-b4ee-001da66c14f3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "raw_data = pd.read_csv('/content/covid19-ct-scans/metadata.csv')\n",
        "raw_data = raw_data.replace('../input/covid19-ct-scans/','/content/covid19-ct-scans/',regex=True)\n",
        "raw_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SidqzfsbWwmt",
        "outputId": "ec71c33d-be9d-4b68-adbb-8eb561aba302"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "raw_data.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DmQqwE1Ww98"
      },
      "outputs": [],
      "source": [
        "def read_nii_demo(filepath, data):\n",
        "    '''\n",
        "    Reads .nii file and returns pixel array\n",
        "    '''\n",
        "    ct_scan = nib.load(filepath)\n",
        "    array   = ct_scan.get_fdata()\n",
        "    array   = np.rot90(np.array(array))\n",
        "    slices = array.shape[2]\n",
        "    array = array[:,:,round(slices*0.2):round(slices*0.8)]\n",
        "    array = np.reshape(np.rollaxis(array, 2),(array.shape[2],array.shape[0],array.shape[1], 1))\n",
        "\n",
        "    for img_no in range(0, array.shape[0]):\n",
        "        # array = Image.resize(array[...,img_no], (img_size,img_size))\n",
        "        img = cv2.resize(array[img_no], dsize=(img_size, img_size), interpolation=cv2.INTER_AREA)\n",
        "        xmax, xmin = img.max(), img.min()\n",
        "        img = (img - xmin)/(xmax - xmin)\n",
        "        data.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94p4Baj8W0A3"
      },
      "outputs": [],
      "source": [
        "all_points1 = []\n",
        "all_points2 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdAQF9xHW3Bo"
      },
      "outputs": [],
      "source": [
        "def read_nii(filepath, data, string):\n",
        "    '''\n",
        "    Reads .nii file and returns pixel array\n",
        "\n",
        "    '''\n",
        "    global all_points1\n",
        "    global all_points2\n",
        "    ct_scan = nib.load(filepath)\n",
        "    array   = ct_scan.get_fdata()\n",
        "    array   = np.rot90(np.array(array))\n",
        "    slices = array.shape[2]\n",
        "    array = array[:,:,round(slices*0.2):round(slices*0.8)]\n",
        "    array = np.reshape(np.rollaxis(array, 2),(array.shape[2],array.shape[0],array.shape[1],1))\n",
        "    #print(array.shape[2])\n",
        "    #array = skimage.transform.resize(array, (array.shape[2], img_size, img_size))\n",
        "    #array = cv2.resize(array, dsize=(img_size, img_size), interpolation=cv2.INTER_CUBIC)\n",
        "    \n",
        "\n",
        "    if string == \"lungs\":\n",
        "      all_points1 = []\n",
        "      all_points2 = []\n",
        "\n",
        "    for img_no in range(0, array.shape[0]):\n",
        "        if string == 'lungs' and np.unique(array[img_no]).size == 1:\n",
        "          continue\n",
        "        img = cv2.resize(array[img_no], dsize=(img_size, img_size), interpolation=cv2.INTER_AREA)\n",
        "        xmax, xmin = img.max(), img.min()\n",
        "        img = (img - xmin)/(xmax - xmin)\n",
        "\n",
        "        if string == 'lungs':\n",
        "          # img = np.uint8(img*255) \n",
        "          img[img>0]=1\n",
        "          img, points1, points2 = cropper(img, demo = 0)\n",
        "          all_points1.append((points1[0], points1[1], points1[2], points1[3]))\n",
        "          all_points2.append((points2[0], points2[1], points2[2], points2[3]))\n",
        "          continue \n",
        "\n",
        "        if string == \"cts\" and img_no < len(all_points1):\n",
        "          img = clahe_enhancer(img, demo = 0)\n",
        "          # img, points1, points2 = cropper(img, demo = 0)\n",
        "          # all_points1.append((points1[0], points1[1], points1[2], points1[3]))\n",
        "          # all_points2.append((points2[0], points2[1], points2[2], points2[3]))   \n",
        "          a,b,c,d = all_points1[img_no]\n",
        "          e,f,g,h = all_points2[img_no]\n",
        "          img1 = img[b:b+d, a:a+c]\n",
        "          img1 = cv2.resize(img1, dsize=(125,250), interpolation=cv2.INTER_AREA)\n",
        "          img2 = img[f:f+h, e:e+g]\n",
        "          img2 = cv2.resize(img2, dsize=(125,250), interpolation=cv2.INTER_AREA)\n",
        "          img = np.concatenate((img1, img2), axis=1)    \n",
        "\n",
        "        if string == \"infections\" and img_no < len(all_points1):\n",
        "          a,b,c,d = all_points1[img_no]\n",
        "          e,f,g,h = all_points2[img_no]\n",
        "          img = np.uint8(img*255)\n",
        "          img1 = img[b:b+d, a:a+c]\n",
        "          img1 = cv2.resize(img1, dsize=(125,250), interpolation=cv2.INTER_AREA)\n",
        "          img2 = img[f:f+h, e:e+g]\n",
        "          img2 = cv2.resize(img2, dsize=(125,250), interpolation=cv2.INTER_AREA)\n",
        "          img = np.concatenate((img1, img2), axis=1)\n",
        "\n",
        "\n",
        "        # img = cv2.resize(img, dsize=(192, 192), interpolation=cv2.INTER_LINEAR)\n",
        "        # img = img/255\n",
        "        #  remember to normalize again\n",
        "        # also resize images and masks for all\n",
        "        \n",
        "        data.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOR61TViW5mX"
      },
      "outputs": [],
      "source": [
        "def cropper(test_img, demo):\n",
        "\n",
        "  test_img = test_img*255\n",
        "  test_img = np.uint8(test_img)\n",
        "\n",
        "  # ret, thresh = cv2.threshold(test_img, 50, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) \n",
        "  # ret, thresh = cv2.threshold(test_img, ret, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) \n",
        "\n",
        "  contours,hierarchy = cv2.findContours(test_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  areas = [cv2.contourArea(c) for c in contours]\n",
        "\n",
        "  x = np.argsort(areas)\n",
        "\n",
        "  max_index = x[x.size - 1]\n",
        "  cnt1=contours[max_index]\n",
        "  second_max_index = x[x.size - 2]\n",
        "  cnt2 = contours[second_max_index]\n",
        "\n",
        "  # max_index = np.argmax(areas)\n",
        "  # cnt=contours[max_index]\n",
        "\n",
        "  x,y,w,h = cv2.boundingRect(cnt1)\n",
        "  p,q,r,s = cv2.boundingRect(cnt2)\n",
        "\n",
        "  cropped1 = test_img[y:y+h, x:x+w]\n",
        "  cropped1 = cv2.resize(cropped1, dsize=(125,250), interpolation=cv2.INTER_AREA)\n",
        "  cropped2 = test_img[q:q+s, p:p+r]\n",
        "  cropped2 = cv2.resize(cropped2, dsize=(125,250), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "  fused = np.concatenate((cropped1, cropped2), axis=1)\n",
        "\n",
        "  # super_cropped = test_img[y+7:y+h-20, x+25:x+w-25]\n",
        "  points_lung1 = []\n",
        "  points_lung2 = []\n",
        "\n",
        "  points_lung1.append(x); points_lung1.append(y); points_lung1.append(w); points_lung1.append(h)\n",
        "  points_lung2.append(p); points_lung2.append(q); points_lung2.append(r); points_lung2.append(s)\n",
        "  \n",
        "  if demo == 1:\n",
        "\n",
        "    fig = plt.figure()\n",
        "    rcParams['figure.figsize'] = 35, 35\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(test_img, cmap='bone')\n",
        "    plt.title(\"Original CT-Scan\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(thresh, cmap='bone')\n",
        "    plt.title(\"Binary Mask\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(fused, cmap='bone')\n",
        "    plt.title(\"Cropped CT scan after making bounding rectangle\")\n",
        "\n",
        "    # plt.subplot(1, 4, 4)\n",
        "    # plt.imshow(super_cropped, cmap='bone')\n",
        "    # plt.title(\"Cropped further manually\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "  return(fused, points_lung1, points_lung2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWuYYw3fW84v"
      },
      "outputs": [],
      "source": [
        "def clahe_enhancer(test_img, demo):\n",
        "\n",
        "  test_img = test_img*255\n",
        "  test_img = np.uint8(test_img)\n",
        "  test_img_flattened = test_img.flatten()\n",
        "  \n",
        "  clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "  clahe_image = clahe.apply(test_img)\n",
        "  clahe_image_flattened = clahe_image.flatten()\n",
        "\n",
        "  if demo == 1:\n",
        "\n",
        "    fig = plt.figure()\n",
        "    rcParams['figure.figsize'] = 10,10\n",
        "    \n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.imshow(test_img, cmap='bone')\n",
        "    plt.title(\"Original CT-Scan\")\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.hist(test_img_flattened)\n",
        "    plt.title(\"Histogram of Original CT-Scan\")\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.imshow(clahe_image, cmap='bone')\n",
        "    plt.title(\"CLAHE Enhanced CT-Scan\")\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.hist(clahe_image_flattened)\n",
        "    plt.title(\"Histogram of CLAHE Enhanced CT-Scan\")\n",
        "\n",
        "  return(clahe_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxXCPds6W_1E"
      },
      "outputs": [],
      "source": [
        "cts = []\n",
        "lungs = []\n",
        "infections = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paJY8A3tXCIF"
      },
      "outputs": [],
      "source": [
        "img_size = 512\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-fL0xsvXEHv",
        "outputId": "d1d9054b-bfe0-40fc-9a37-43f2f3f44836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-037eef1727f5>:28: RuntimeWarning: invalid value encountered in true_divide\n",
            "  img = (img - xmin)/(xmax - xmin)\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, 20):\n",
        "    read_nii(raw_data.loc[i,'lung_mask'], lungs, 'lungs')\n",
        "    read_nii(raw_data.loc[i,'ct_scan'], cts, 'cts') \n",
        "    read_nii(raw_data.loc[i,'infection_mask'], infections, 'infections')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnU4Wb0xXGAb",
        "outputId": "a1b392f5-281c-4c2a-91b3-d0e71b83cc1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(infections)/len(cts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpT3fGDbXOc6"
      },
      "outputs": [],
      "source": [
        "new_dim = 224\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpY3Uaz7XQBQ"
      },
      "outputs": [],
      "source": [
        "for i in range(0,len(cts)):\n",
        "  cts[i] = cv2.resize(cts[i], dsize=(new_dim, new_dim), interpolation=cv2.INTER_LINEAR)\n",
        "  # cts[i] = cts[i]/255\n",
        "  infections[i] = cv2.resize(infections[i], dsize=(new_dim, new_dim), interpolation=cv2.INTER_LINEAR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE1FQJcRXRt7"
      },
      "outputs": [],
      "source": [
        "cts = np.array(cts)\n",
        "infections = np.array(infections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsqKxzr7XTe6"
      },
      "outputs": [],
      "source": [
        "cts = np.uint8(cts)\n",
        "infections = np.uint8(infections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaTRwHk7XVhj"
      },
      "outputs": [],
      "source": [
        "cts = cts/255\n",
        "infections = infections/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIivEHoMXXvR"
      },
      "outputs": [],
      "source": [
        "cts = cts.reshape(len(cts), new_dim, new_dim, 1)\n",
        "infections = infections.reshape(len(infections), new_dim, new_dim, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT1tCwJGXZzn"
      },
      "outputs": [],
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(cts, infections, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImAdAR3JYNEJ",
        "outputId": "e98fbf2f-0f94-467f-f0d0-e73a268bcaac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1478, 224, 224, 1) (634, 224, 224, 1)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape, x_valid.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDJ6jdfrYOnN",
        "outputId": "052ae592-ac55-4744-eef8-41be5c6fc615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 224, 224, 32) 320         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 224, 224, 32) 9248        conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 224, 224, 32) 128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 112, 112, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 112, 112, 32) 0           max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 112, 112, 64) 18496       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 112, 112, 64) 36928       conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 112, 112, 64) 256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 56, 56, 64)   0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 56, 56, 128)  73856       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 56, 56, 128)  147584      conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 56, 56, 128)  512         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 28, 28, 128)  0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 28, 28, 256)  295168      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 28, 28, 256)  590080      conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 28, 28, 256)  1024        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 256)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 14, 14, 256)  0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 14, 14, 512)  1180160     dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 14, 14, 512)  2359808     conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 28, 28, 256)  524544      conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 28, 28, 512)  0           conv2d_transpose[0][0]           \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 28, 28, 512)  2048        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 28, 28, 256)  1179904     batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 28, 28, 256)  590080      conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 56, 56, 128)  131200      conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 56, 56, 256)  0           conv2d_transpose_1[0][0]         \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 56, 56, 256)  1024        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 56, 56, 128)  295040      batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 56, 56, 128)  147584      conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 112, 112, 64) 32832       conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 112, 112, 128 0           conv2d_transpose_2[0][0]         \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 112, 112, 128 512         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 112, 112, 64) 73792       batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 112, 112, 64) 36928       conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 224, 224, 32) 8224        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 224, 224, 64) 0           conv2d_transpose_3[0][0]         \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 224, 224, 64) 256         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 224, 224, 32) 18464       batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 224, 224, 32) 9248        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 224, 224, 1)  33          conv2d_17[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 7,765,281\n",
            "Trainable params: 7,762,401\n",
            "Non-trainable params: 2,880\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = Input((new_dim, new_dim, 1))\n",
        "# s = Lambda(lambda x: x / 255) (inputs)\n",
        "\n",
        "# def mish(inputs):\n",
        "#     return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
        "    \n",
        "c1 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (inputs)\n",
        "c1 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (c1)\n",
        "c1 = BatchNormalization()(c1)\n",
        "p1 = MaxPooling2D((2, 2)) (c1)\n",
        "p1 = Dropout(0.25)(p1)\n",
        "\n",
        "c2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (p1)\n",
        "c2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (c2)\n",
        "c2 = BatchNormalization()(c2)\n",
        "p2 = MaxPooling2D((2, 2)) (c2)\n",
        "p2 = Dropout(0.25)(p2)\n",
        "\n",
        "c3 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (p2)\n",
        "c3 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (c3)\n",
        "c3 = BatchNormalization()(c3)\n",
        "p3 = MaxPooling2D((2, 2)) (c3)\n",
        "p3 = Dropout(0.25)(p3)\n",
        "\n",
        "c4 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (p3)\n",
        "c4 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (c4)\n",
        "c4 = BatchNormalization()(c4)\n",
        "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "p4 = Dropout(0.25)(p4)\n",
        "\n",
        "c5 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (p4)\n",
        "c5 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (c5)\n",
        "\n",
        "u6 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = concatenate([u6, c4])\n",
        "u6 = BatchNormalization()(u6)\n",
        "c6 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (u6)\n",
        "c6 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (c6)\n",
        "\n",
        "\n",
        "u7 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "u7 = concatenate([u7, c3])\n",
        "u7 = BatchNormalization()(u7)\n",
        "c7 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (u7)\n",
        "c7 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (c7)\n",
        "\n",
        "\n",
        "u8 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "u8 = concatenate([u8, c2])\n",
        "u8 = BatchNormalization()(u8)\n",
        "c8 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (u8)\n",
        "c8 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (c8)\n",
        "\n",
        "\n",
        "u9 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c8)\n",
        "u9 = concatenate([u9, c1], axis=3)\n",
        "u9 = BatchNormalization()(u9)\n",
        "c9 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (u9)\n",
        "c9 = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=\"he_normal\") (c9)\n",
        "\n",
        "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
        "\n",
        "model = Model(inputs=[inputs], outputs=[outputs])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_VI2KOEYRYD"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJJ-OOvMYU65"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    loss = 0.5*binary_crossentropy(y_true, y_pred) + 0.5*dice_loss(y_true, y_pred)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCRIroUKYWlm"
      },
      "outputs": [],
      "source": [
        "def dice_coeff(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2Pqy4hrYdLR"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=bce_dice_loss,\n",
        "              metrics=[dice_coeff])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uwLOkzaYfue",
        "outputId": "cf9f92ab-3175-4f3f-8955-24b843255c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ],
      "source": [
        "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto', period=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ijrblkApyZfH",
        "outputId": "b5d70780-671b-42f2-e4c8-86f143fb42fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "13ruJAv4ywAy",
        "outputId": "c7a292e0-c850-4ac5-a3c6-b1ed32e2c2ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "'/device:GPU:0'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x=x_train, y=y_train, batch_size=32, epochs=80, verbose=1,callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M-OhJ7QgOzQ",
        "outputId": "cf6b5b3c-eda8-4b6a-dac3-848b12bbd3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "47/47 [==============================] - ETA: 0s - loss: 0.4613 - dice_coeff: 0.2732 \n",
            "Epoch 00001: loss improved from inf to 0.46128, saving model to best_model.hdf5\n",
            "47/47 [==============================] - 1806s 38s/step - loss: 0.4613 - dice_coeff: 0.2732\n",
            "Epoch 2/80\n",
            "47/47 [==============================] - ETA: 0s - loss: 0.2247 - dice_coeff: 0.6644 \n",
            "Epoch 00002: loss improved from 0.46128 to 0.22468, saving model to best_model.hdf5\n",
            "47/47 [==============================] - 1767s 38s/step - loss: 0.2247 - dice_coeff: 0.6644\n",
            "Epoch 3/80\n",
            "47/47 [==============================] - ETA: 0s - loss: 0.1803 - dice_coeff: 0.7283 \n",
            "Epoch 00003: loss improved from 0.22468 to 0.18030, saving model to best_model.hdf5\n",
            "47/47 [==============================] - 1775s 38s/step - loss: 0.1803 - dice_coeff: 0.7283\n",
            "Epoch 4/80\n",
            "47/47 [==============================] - ETA: 0s - loss: 0.1639 - dice_coeff: 0.7531 \n",
            "Epoch 00004: loss improved from 0.18030 to 0.16394, saving model to best_model.hdf5\n",
            "47/47 [==============================] - 1735s 37s/step - loss: 0.1639 - dice_coeff: 0.7531\n",
            "Epoch 5/80\n",
            "47/47 [==============================] - ETA: 0s - loss: 0.1548 - dice_coeff: 0.7682 \n",
            "Epoch 00005: loss improved from 0.16394 to 0.15484, saving model to best_model.hdf5\n",
            "47/47 [==============================] - 1728s 37s/step - loss: 0.1548 - dice_coeff: 0.7682\n",
            "Epoch 6/80\n",
            "47/47 [==============================] - ETA: 0s - loss: 0.1547 - dice_coeff: 0.7675 \n",
            "Epoch 00006: loss improved from 0.15484 to 0.15467, saving model to best_model.hdf5\n",
            "47/47 [==============================] - 1747s 37s/step - loss: 0.1547 - dice_coeff: 0.7675\n",
            "Epoch 7/80\n",
            "47/47 [==============================] - ETA: 0s - loss: 0.1403 - dice_coeff: 0.7861 \n",
            "Epoch 00007: loss improved from 0.15467 to 0.14034, saving model to best_model.hdf5\n",
            "47/47 [==============================] - 1714s 36s/step - loss: 0.1403 - dice_coeff: 0.7861\n",
            "Epoch 8/80\n",
            " 9/47 [====>.........................] - ETA: 22:01 - loss: 0.1554 - dice_coeff: 0.7632"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(model.history.history['loss'])\n",
        "\n",
        "plt.plot(model.history.history['loss'])\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.ylim(0,0)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('model loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(model.history.history['dice_coeff'])\n",
        "plt.plot(model.history.history['dice_coeff'])\n",
        "plt.title('dice score')\n",
        "plt.ylim(0, 0)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('dice coefficient')\n",
        "plt.legend(['train', 'validation'], loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jfEMDOCu8SFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUkd0sWFYkqI"
      },
      "outputs": [],
      "source": [
        "model.load_weights('best_model.hdf5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NooseNdZYoU0"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(x_valid, y_valid, batch_size=32)\n",
        "print(\"test loss, test dice coefficient:\", score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyGyl9usYspw"
      },
      "outputs": [],
      "source": [
        "def compare_actual_and_predicted(image_no):\n",
        "    temp = model.predict(cts[image_no].reshape(1,new_dim, new_dim, 1))\n",
        "\n",
        "    fig = plt.figure(figsize=(15,15))\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.imshow(cts[image_no].reshape(new_dim, new_dim), cmap='bone')\n",
        "    plt.title('Original Image (CT)')\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.imshow(infections[image_no].reshape(new_dim,new_dim), cmap='bone')\n",
        "    plt.title('Actual mask')\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.imshow(temp.reshape(new_dim,new_dim), cmap='bone')\n",
        "    plt.title('Predicted mask')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7ZA1fsbYtFO"
      },
      "outputs": [],
      "source": [
        "for i in [40,50,60,70, 355, 380, 100]:\n",
        "    compare_actual_and_predicted(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ-LH43MyfvZ"
      },
      "source": [
        "https://github.com/Sanjanav-98/COVID-19-Detection-and-Lung-Segmentation-using-UNet/blob/main/Covid_19_Infection_area_detection_using_unet.ipynb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}